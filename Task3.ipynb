{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Steps in Image Recognition"
   ]
  },
 
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Problem setup**\n",
    "\n",
    "In this assignment you are asked to apply ANNs in order to classify a given silhouette (image) of a vehicle as one of four types\n",
    "-\ta double decker bus (class 0),\n",
    "-\tOpel Manta 400 (class 1),\n",
    "-\tSaab 9000 (class 2), \n",
    "-\tCheverolet van (class 3),\n",
    "\n",
    "using a set of features extracted from the silhouette. The vehicle may be viewed from one of many different angles. The dataset is given in two files: **veh_train.csv** and **veh_test.csv**. Variable class is the outcome (nominal) variable.\n",
    "\n",
    "One of the principal questions of this assignment is whether PCA can help (or, at least, not make worse) predicting classes.\n",
    "\n",
    "Data source and detailed description: https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29 \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load traditional libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.decomposition import PCA as skPCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# load KERAS\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set constants\n",
    "c_nclass = 4\n",
    "\n",
    "list_feats = [\"V1\", \"V2\", \"V3\",\"V4\",\"V5\",\"V6\",\"V7\",\"V8\",\"V9\",\"V10\",\"V11\",\"V12\",\"V13\",\"V14\",\"V15\",\"V16\",\n",
    "                             \"V17\",\"V18\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function for your ANN\n",
    "def myANN(input_dim):\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=input_dim, activation='relu'))\n",
    "    model.add(Dense(c_nclass, activation=\"softmax\"))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data\n",
    "veh_train = pd.read_csv('veh_train.csv', header=0)\n",
    "veh_test = pd.read_csv('veh_test.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the training and the test samples\n",
    "X_train = veh_train.drop('class', axis=1)\n",
    "y_train = veh_train['class']\n",
    "\n",
    "X_test = veh_test.drop('class', axis=1)\n",
    "y_test = veh_test['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.142657</td>\n",
       "      <td>0.493358</td>\n",
       "      <td>0.048570</td>\n",
       "      <td>0.266144</td>\n",
       "      <td>1.350583</td>\n",
       "      <td>0.331154</td>\n",
       "      <td>-0.211507</td>\n",
       "      <td>0.142691</td>\n",
       "      <td>-0.233227</td>\n",
       "      <td>0.751569</td>\n",
       "      <td>-0.406215</td>\n",
       "      <td>-0.350463</td>\n",
       "      <td>0.268827</td>\n",
       "      <td>-0.318038</td>\n",
       "      <td>-0.084475</td>\n",
       "      <td>0.389751</td>\n",
       "      <td>-0.334073</td>\n",
       "      <td>0.170051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.241760</td>\n",
       "      <td>0.817326</td>\n",
       "      <td>1.519254</td>\n",
       "      <td>1.204229</td>\n",
       "      <td>0.563227</td>\n",
       "      <td>0.331154</td>\n",
       "      <td>1.140490</td>\n",
       "      <td>-1.135304</td>\n",
       "      <td>0.923806</td>\n",
       "      <td>0.682146</td>\n",
       "      <td>1.099655</td>\n",
       "      <td>1.094856</td>\n",
       "      <td>1.360598</td>\n",
       "      <td>0.096104</td>\n",
       "      <td>1.551923</td>\n",
       "      <td>-0.418356</td>\n",
       "      <td>-0.172632</td>\n",
       "      <td>0.036608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.101589</td>\n",
       "      <td>-0.640530</td>\n",
       "      <td>-0.015373</td>\n",
       "      <td>-0.308812</td>\n",
       "      <td>0.169549</td>\n",
       "      <td>0.103675</td>\n",
       "      <td>-0.752306</td>\n",
       "      <td>0.653888</td>\n",
       "      <td>-0.618905</td>\n",
       "      <td>-0.359197</td>\n",
       "      <td>-0.918852</td>\n",
       "      <td>-0.745667</td>\n",
       "      <td>-1.459809</td>\n",
       "      <td>-1.284369</td>\n",
       "      <td>-0.084475</td>\n",
       "      <td>-0.302912</td>\n",
       "      <td>1.603217</td>\n",
       "      <td>1.504479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.078570</td>\n",
       "      <td>-0.154578</td>\n",
       "      <td>-0.782686</td>\n",
       "      <td>1.083186</td>\n",
       "      <td>5.418592</td>\n",
       "      <td>9.885314</td>\n",
       "      <td>-0.602084</td>\n",
       "      <td>0.526089</td>\n",
       "      <td>-0.618905</td>\n",
       "      <td>-0.289774</td>\n",
       "      <td>1.676371</td>\n",
       "      <td>-0.655335</td>\n",
       "      <td>0.390135</td>\n",
       "      <td>7.550654</td>\n",
       "      <td>0.529174</td>\n",
       "      <td>-0.187469</td>\n",
       "      <td>-1.464159</td>\n",
       "      <td>-1.698148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.608128</td>\n",
       "      <td>1.951214</td>\n",
       "      <td>1.519254</td>\n",
       "      <td>0.084579</td>\n",
       "      <td>-1.536390</td>\n",
       "      <td>-0.578765</td>\n",
       "      <td>2.582619</td>\n",
       "      <td>-1.902101</td>\n",
       "      <td>2.852195</td>\n",
       "      <td>1.445798</td>\n",
       "      <td>2.925923</td>\n",
       "      <td>2.912796</td>\n",
       "      <td>2.694984</td>\n",
       "      <td>1.752670</td>\n",
       "      <td>-0.289024</td>\n",
       "      <td>-0.418356</td>\n",
       "      <td>-1.302718</td>\n",
       "      <td>-1.698148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.467957</td>\n",
       "      <td>-0.316562</td>\n",
       "      <td>-1.038457</td>\n",
       "      <td>-0.369334</td>\n",
       "      <td>0.432001</td>\n",
       "      <td>0.103675</td>\n",
       "      <td>-0.962616</td>\n",
       "      <td>0.909487</td>\n",
       "      <td>-1.004582</td>\n",
       "      <td>-0.150928</td>\n",
       "      <td>-0.854772</td>\n",
       "      <td>-0.903749</td>\n",
       "      <td>-0.337712</td>\n",
       "      <td>-0.732180</td>\n",
       "      <td>-0.698124</td>\n",
       "      <td>-1.111019</td>\n",
       "      <td>0.634572</td>\n",
       "      <td>0.837265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  0.142657  0.493358  0.048570  0.266144  1.350583  0.331154 -0.211507   \n",
       "1  1.241760  0.817326  1.519254  1.204229  0.563227  0.331154  1.140490   \n",
       "2 -0.101589 -0.640530 -0.015373 -0.308812  0.169549  0.103675 -0.752306   \n",
       "3 -1.078570 -0.154578 -0.782686  1.083186  5.418592  9.885314 -0.602084   \n",
       "4  1.608128  1.951214  1.519254  0.084579 -1.536390 -0.578765  2.582619   \n",
       "5 -0.467957 -0.316562 -1.038457 -0.369334  0.432001  0.103675 -0.962616   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.142691 -0.233227  0.751569 -0.406215 -0.350463  0.268827 -0.318038   \n",
       "1 -1.135304  0.923806  0.682146  1.099655  1.094856  1.360598  0.096104   \n",
       "2  0.653888 -0.618905 -0.359197 -0.918852 -0.745667 -1.459809 -1.284369   \n",
       "3  0.526089 -0.618905 -0.289774  1.676371 -0.655335  0.390135  7.550654   \n",
       "4 -1.902101  2.852195  1.445798  2.925923  2.912796  2.694984  1.752670   \n",
       "5  0.909487 -1.004582 -0.150928 -0.854772 -0.903749 -0.337712 -0.732180   \n",
       "\n",
       "        V15       V16       V17       V18  \n",
       "0 -0.084475  0.389751 -0.334073  0.170051  \n",
       "1  1.551923 -0.418356 -0.172632  0.036608  \n",
       "2 -0.084475 -0.302912  1.603217  1.504479  \n",
       "3  0.529174 -0.187469 -1.464159 -1.698148  \n",
       "4 -0.289024 -0.418356 -1.302718 -1.698148  \n",
       "5 -0.698124 -1.111019  0.634572  0.837265  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since PCA will be applied, let's scale the features\n",
    "\n",
    "# scale the training data\n",
    "X_train_sc = StandardScaler().fit_transform(X_train)\n",
    "X_train_sc = pd.DataFrame(X_train_sc)\n",
    "X_train_sc.columns = list_feats\n",
    "\n",
    "# scale the test data\n",
    "X_test_sc = StandardScaler().fit_transform(X_test)\n",
    "X_test_sc = pd.DataFrame(X_test_sc)\n",
    "X_test_sc.columns = list_feats\n",
    "\n",
    "# preview the scaled data\n",
    "X_train_sc.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 (1 point). Descriptive analysis of the ORIGINAL training sample\n",
    "\n",
    "Read the description of the features. Analyze the structure and features of the dataset (no graphs!). Do the following on the **training sample**:\n",
    "\n",
    "-\tCheck whether the classes are balanced.\n",
    "-\tCalculate summary statistics and correlations of the ORIGINAL features.\n",
    "-\tDo you expect that PCA may be helpful? Give brief comments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.264012\n",
       "2    0.256637\n",
       "1    0.243363\n",
       "3    0.235988\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the balance of classes\n",
    "veh_train['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.279762\n",
       "2    0.255952\n",
       "3    0.232143\n",
       "0    0.232143\n",
       "Name: class, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "veh_test['class'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that in both samplings - train and test, the classes are balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "      <td>678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>93.831858</td>\n",
       "      <td>44.954277</td>\n",
       "      <td>82.240413</td>\n",
       "      <td>169.205015</td>\n",
       "      <td>61.707965</td>\n",
       "      <td>8.544248</td>\n",
       "      <td>169.039823</td>\n",
       "      <td>40.883481</td>\n",
       "      <td>20.604720</td>\n",
       "      <td>148.174041</td>\n",
       "      <td>188.678466</td>\n",
       "      <td>441.075221</td>\n",
       "      <td>175.135693</td>\n",
       "      <td>72.303835</td>\n",
       "      <td>6.412979</td>\n",
       "      <td>12.623894</td>\n",
       "      <td>189.069322</td>\n",
       "      <td>195.725664</td>\n",
       "      <td>1.464602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.194536</td>\n",
       "      <td>6.178007</td>\n",
       "      <td>15.650532</td>\n",
       "      <td>33.070412</td>\n",
       "      <td>7.626063</td>\n",
       "      <td>4.399237</td>\n",
       "      <td>33.308680</td>\n",
       "      <td>7.830535</td>\n",
       "      <td>2.594752</td>\n",
       "      <td>14.415109</td>\n",
       "      <td>31.234235</td>\n",
       "      <td>177.254322</td>\n",
       "      <td>32.998311</td>\n",
       "      <td>7.249246</td>\n",
       "      <td>4.892397</td>\n",
       "      <td>8.668615</td>\n",
       "      <td>6.198792</td>\n",
       "      <td>7.499382</td>\n",
       "      <td>1.118298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>73.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>181.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>88.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>141.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>167.250000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>190.250000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>93.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>365.500000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>195.750000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>198.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>217.000000</td>\n",
       "      <td>586.750000</td>\n",
       "      <td>198.750000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>201.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>117.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>110.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>133.000000</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>265.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>288.000000</td>\n",
       "      <td>1018.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>211.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               V1          V2          V3          V4          V5          V6  \\\n",
       "count  678.000000  678.000000  678.000000  678.000000  678.000000  678.000000   \n",
       "mean    93.831858   44.954277   82.240413  169.205015   61.707965    8.544248   \n",
       "std      8.194536    6.178007   15.650532   33.070412    7.626063    4.399237   \n",
       "min     73.000000   33.000000   40.000000  105.000000   47.000000    2.000000   \n",
       "25%     88.000000   40.000000   70.000000  141.000000   57.000000    7.000000   \n",
       "50%     93.000000   44.000000   80.000000  168.000000   61.000000    8.000000   \n",
       "75%    100.000000   49.000000   98.000000  195.750000   65.000000   10.000000   \n",
       "max    117.000000   59.000000  110.000000  322.000000  133.000000   52.000000   \n",
       "\n",
       "               V7          V8          V9         V10         V11  \\\n",
       "count  678.000000  678.000000  678.000000  678.000000  678.000000   \n",
       "mean   169.039823   40.883481   20.604720  148.174041  188.678466   \n",
       "std     33.308680    7.830535    2.594752   14.415109   31.234235   \n",
       "min    112.000000   26.000000   17.000000  119.000000  130.000000   \n",
       "25%    147.000000   33.000000   19.000000  137.000000  167.250000   \n",
       "50%    157.000000   43.000000   20.000000  146.000000  179.000000   \n",
       "75%    198.000000   46.000000   23.000000  159.000000  217.000000   \n",
       "max    265.000000   61.000000   29.000000  188.000000  288.000000   \n",
       "\n",
       "               V12         V13         V14         V15         V16  \\\n",
       "count   678.000000  678.000000  678.000000  678.000000  678.000000   \n",
       "mean    441.075221  175.135693   72.303835    6.412979   12.623894   \n",
       "std     177.254322   32.998311    7.249246    4.892397    8.668615   \n",
       "min     184.000000  109.000000   59.000000    0.000000    0.000000   \n",
       "25%     320.000000  149.000000   67.000000    2.000000    6.000000   \n",
       "50%     365.500000  174.000000   71.000000    6.000000   11.000000   \n",
       "75%     586.750000  198.750000   75.000000    9.000000   18.000000   \n",
       "max    1018.000000  268.000000  127.000000   22.000000   41.000000   \n",
       "\n",
       "              V17         V18       class  \n",
       "count  678.000000  678.000000  678.000000  \n",
       "mean   189.069322  195.725664    1.464602  \n",
       "std      6.198792    7.499382    1.118298  \n",
       "min    176.000000  181.000000    0.000000  \n",
       "25%    185.000000  190.250000    0.000000  \n",
       "50%    188.000000  197.000000    1.000000  \n",
       "75%    193.000000  201.000000    2.000000  \n",
       "max    206.000000  211.000000    3.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary statistics on features\n",
    "veh_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>V1</th>\n",
       "      <td>1.00</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-0.79</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.59</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V2</th>\n",
       "      <td>0.69</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.86</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V3</th>\n",
       "      <td>0.79</td>\n",
       "      <td>0.79</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.91</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V4</th>\n",
       "      <td>0.70</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.78</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.75</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.55</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V5</th>\n",
       "      <td>0.10</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.65</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.11</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V6</th>\n",
       "      <td>0.15</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.63</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V7</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.91</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V8</th>\n",
       "      <td>-0.79</td>\n",
       "      <td>-0.83</td>\n",
       "      <td>-0.91</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V9</th>\n",
       "      <td>0.81</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.72</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.99</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V10</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.81</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.81</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V11</th>\n",
       "      <td>0.77</td>\n",
       "      <td>0.82</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.96</td>\n",
       "      <td>-0.95</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.96</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V12</th>\n",
       "      <td>0.82</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.73</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.96</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V13</th>\n",
       "      <td>0.59</td>\n",
       "      <td>0.94</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.80</td>\n",
       "      <td>-0.77</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V14</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.22</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.27</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>-0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V15</th>\n",
       "      <td>0.23</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.05</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.11</td>\n",
       "      <td>1.00</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V16</th>\n",
       "      <td>0.15</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.17</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V17</th>\n",
       "      <td>0.29</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>-0.02</td>\n",
       "      <td>-0.09</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>-0.13</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>-0.25</td>\n",
       "      <td>-0.78</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>V18</th>\n",
       "      <td>0.35</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.09</td>\n",
       "      <td>-0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.14</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.90</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       V1    V2    V3    V4    V5    V6    V7    V8    V9   V10   V11   V12  \\\n",
       "V1   1.00  0.69  0.79  0.70  0.10  0.15  0.81 -0.79  0.81  0.67  0.77  0.82   \n",
       "V2   0.69  1.00  0.79  0.63  0.15  0.25  0.86 -0.83  0.86  0.96  0.82  0.85   \n",
       "V3   0.79  0.79  1.00  0.78  0.16  0.27  0.91 -0.91  0.89  0.77  0.87  0.89   \n",
       "V4   0.70  0.63  0.78  1.00  0.65  0.43  0.75 -0.80  0.72  0.57  0.79  0.73   \n",
       "V5   0.10  0.15  0.16  0.65  1.00  0.63  0.11 -0.19  0.09  0.13  0.24  0.09   \n",
       "V6   0.15  0.25  0.27  0.43  0.63  1.00  0.17 -0.19  0.17  0.32  0.29  0.15   \n",
       "V7   0.81  0.86  0.91  0.75  0.11  0.17  1.00 -0.97  0.99  0.81  0.96  1.00   \n",
       "V8  -0.79 -0.83 -0.91 -0.80 -0.19 -0.19 -0.97  1.00 -0.95 -0.77 -0.95 -0.96   \n",
       "V9   0.81  0.86  0.89  0.72  0.09  0.17  0.99 -0.95  1.00  0.81  0.95  0.99   \n",
       "V10  0.67  0.96  0.77  0.57  0.13  0.32  0.81 -0.77  0.81  1.00  0.75  0.79   \n",
       "V11  0.77  0.82  0.87  0.79  0.24  0.29  0.96 -0.95  0.95  0.75  1.00  0.96   \n",
       "V12  0.82  0.85  0.89  0.73  0.09  0.15  1.00 -0.96  0.99  0.79  0.96  1.00   \n",
       "V13  0.59  0.94  0.70  0.55  0.12  0.20  0.80 -0.77  0.80  0.86  0.79  0.80   \n",
       "V14 -0.25  0.08 -0.22 -0.21  0.11  0.27 -0.00  0.08  0.01  0.06  0.11  0.01   \n",
       "V15  0.23  0.15  0.12  0.05 -0.08 -0.00  0.07 -0.05  0.08  0.14  0.04  0.08   \n",
       "V16  0.15 -0.04  0.25  0.18 -0.02  0.05  0.19 -0.17  0.19 -0.03  0.17  0.18   \n",
       "V17  0.29 -0.14  0.13  0.37  0.23 -0.05 -0.02 -0.09 -0.04 -0.13 -0.01 -0.01   \n",
       "V18  0.35  0.01  0.31  0.46  0.26  0.13  0.09 -0.19  0.07  0.05  0.05  0.08   \n",
       "\n",
       "      V13   V14   V15   V16   V17   V18  \n",
       "V1   0.59 -0.25  0.23  0.15  0.29  0.35  \n",
       "V2   0.94  0.08  0.15 -0.04 -0.14  0.01  \n",
       "V3   0.70 -0.22  0.12  0.25  0.13  0.31  \n",
       "V4   0.55 -0.21  0.05  0.18  0.37  0.46  \n",
       "V5   0.12  0.11 -0.08 -0.02  0.23  0.26  \n",
       "V6   0.20  0.27 -0.00  0.05 -0.05  0.13  \n",
       "V7   0.80 -0.00  0.07  0.19 -0.02  0.09  \n",
       "V8  -0.77  0.08 -0.05 -0.17 -0.09 -0.19  \n",
       "V9   0.80  0.01  0.08  0.19 -0.04  0.07  \n",
       "V10  0.86  0.06  0.14 -0.03 -0.13  0.05  \n",
       "V11  0.79  0.11  0.04  0.17 -0.01  0.05  \n",
       "V12  0.80  0.01  0.08  0.18 -0.01  0.08  \n",
       "V13  1.00  0.21  0.17 -0.08 -0.25 -0.14  \n",
       "V14  0.21  1.00 -0.11 -0.15 -0.78 -0.84  \n",
       "V15  0.17 -0.11  1.00 -0.06  0.12  0.10  \n",
       "V16 -0.08 -0.15 -0.06  1.00  0.10  0.22  \n",
       "V17 -0.25 -0.78  0.12  0.10  1.00  0.90  \n",
       "V18 -0.14 -0.84  0.10  0.22  0.90  1.00  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#correlation matrix\n",
    "pd.DataFrame.corr(X_train, method=\"pearson\").round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your comments here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 (1 point). ANN on the SCALED full dataset\n",
    "\n",
    "Estimate and validate a simple 1-layer ANN on the SCALED full data. Do the following:\n",
    "\n",
    "- Transform the outcome variable on both sample appropriately.\n",
    "- Run the model (use 100 epochs).\n",
    "- Validate its performance.\n",
    "- Give brief comments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the outcome variable\n",
    "Y_01_train = np_utils.to_categorical(y_train, c_nclass)\n",
    "Y_01_test = np_utils.to_categorical(y_test, c_nclass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setup the Model1 with the myANN function\n",
    "model1 = myANN(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 678 samples, validate on 168 samples\n",
      "Epoch 1/100\n",
      "678/678 [==============================] - 1s 1ms/step - loss: 1.3141 - accuracy: 0.3717 - val_loss: 1.2480 - val_accuracy: 0.4821\n",
      "Epoch 2/100\n",
      "678/678 [==============================] - 0s 89us/step - loss: 1.0855 - accuracy: 0.5501 - val_loss: 1.1148 - val_accuracy: 0.5655\n",
      "Epoch 3/100\n",
      "678/678 [==============================] - 0s 104us/step - loss: 0.9558 - accuracy: 0.6622 - val_loss: 1.0183 - val_accuracy: 0.5952\n",
      "Epoch 4/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.8654 - accuracy: 0.7035 - val_loss: 0.9610 - val_accuracy: 0.6250\n",
      "Epoch 5/100\n",
      "678/678 [==============================] - 0s 108us/step - loss: 0.7930 - accuracy: 0.7286 - val_loss: 0.8912 - val_accuracy: 0.6905\n",
      "Epoch 6/100\n",
      "678/678 [==============================] - 0s 98us/step - loss: 0.7285 - accuracy: 0.7581 - val_loss: 0.8448 - val_accuracy: 0.6964\n",
      "Epoch 7/100\n",
      "678/678 [==============================] - 0s 89us/step - loss: 0.6765 - accuracy: 0.7596 - val_loss: 0.8165 - val_accuracy: 0.7024\n",
      "Epoch 8/100\n",
      "678/678 [==============================] - 0s 91us/step - loss: 0.6347 - accuracy: 0.7773 - val_loss: 0.7674 - val_accuracy: 0.7262\n",
      "Epoch 9/100\n",
      "678/678 [==============================] - 0s 101us/step - loss: 0.6019 - accuracy: 0.7729 - val_loss: 0.7433 - val_accuracy: 0.7321\n",
      "Epoch 10/100\n",
      "678/678 [==============================] - 0s 103us/step - loss: 0.5751 - accuracy: 0.7684 - val_loss: 0.7314 - val_accuracy: 0.7202\n",
      "Epoch 11/100\n",
      "678/678 [==============================] - 0s 91us/step - loss: 0.5466 - accuracy: 0.7832 - val_loss: 0.7018 - val_accuracy: 0.7202\n",
      "Epoch 12/100\n",
      "678/678 [==============================] - 0s 89us/step - loss: 0.5310 - accuracy: 0.7802 - val_loss: 0.6991 - val_accuracy: 0.7024\n",
      "Epoch 13/100\n",
      "678/678 [==============================] - 0s 86us/step - loss: 0.5101 - accuracy: 0.7891 - val_loss: 0.6768 - val_accuracy: 0.7024\n",
      "Epoch 14/100\n",
      "678/678 [==============================] - 0s 85us/step - loss: 0.4904 - accuracy: 0.7994 - val_loss: 0.6743 - val_accuracy: 0.7262\n",
      "Epoch 15/100\n",
      "678/678 [==============================] - 0s 81us/step - loss: 0.4769 - accuracy: 0.8038 - val_loss: 0.6590 - val_accuracy: 0.7262\n",
      "Epoch 16/100\n",
      "678/678 [==============================] - 0s 85us/step - loss: 0.4651 - accuracy: 0.8083 - val_loss: 0.6341 - val_accuracy: 0.7321\n",
      "Epoch 17/100\n",
      "678/678 [==============================] - 0s 135us/step - loss: 0.4490 - accuracy: 0.8171 - val_loss: 0.6230 - val_accuracy: 0.7321\n",
      "Epoch 18/100\n",
      "678/678 [==============================] - 0s 146us/step - loss: 0.4398 - accuracy: 0.8186 - val_loss: 0.6218 - val_accuracy: 0.7321\n",
      "Epoch 19/100\n",
      "678/678 [==============================] - 0s 144us/step - loss: 0.4310 - accuracy: 0.8171 - val_loss: 0.6171 - val_accuracy: 0.7381\n",
      "Epoch 20/100\n",
      "678/678 [==============================] - 0s 87us/step - loss: 0.4216 - accuracy: 0.8260 - val_loss: 0.6073 - val_accuracy: 0.7381\n",
      "Epoch 21/100\n",
      "678/678 [==============================] - 0s 81us/step - loss: 0.4099 - accuracy: 0.8319 - val_loss: 0.5937 - val_accuracy: 0.7679\n",
      "Epoch 22/100\n",
      "678/678 [==============================] - 0s 82us/step - loss: 0.4019 - accuracy: 0.8289 - val_loss: 0.5872 - val_accuracy: 0.7440\n",
      "Epoch 23/100\n",
      "678/678 [==============================] - 0s 120us/step - loss: 0.3930 - accuracy: 0.8392 - val_loss: 0.5858 - val_accuracy: 0.7798\n",
      "Epoch 24/100\n",
      "678/678 [==============================] - 0s 151us/step - loss: 0.3872 - accuracy: 0.8348 - val_loss: 0.5814 - val_accuracy: 0.7619\n",
      "Epoch 25/100\n",
      "678/678 [==============================] - 0s 146us/step - loss: 0.3810 - accuracy: 0.8466 - val_loss: 0.5801 - val_accuracy: 0.7679\n",
      "Epoch 26/100\n",
      "678/678 [==============================] - 0s 99us/step - loss: 0.3810 - accuracy: 0.8333 - val_loss: 0.5640 - val_accuracy: 0.7857\n",
      "Epoch 27/100\n",
      "678/678 [==============================] - 0s 91us/step - loss: 0.3699 - accuracy: 0.8540 - val_loss: 0.5866 - val_accuracy: 0.7321\n",
      "Epoch 28/100\n",
      "678/678 [==============================] - 0s 82us/step - loss: 0.3649 - accuracy: 0.8437 - val_loss: 0.5684 - val_accuracy: 0.7500\n",
      "Epoch 29/100\n",
      "678/678 [==============================] - 0s 98us/step - loss: 0.3572 - accuracy: 0.8496 - val_loss: 0.5567 - val_accuracy: 0.7500\n",
      "Epoch 30/100\n",
      "678/678 [==============================] - 0s 107us/step - loss: 0.3505 - accuracy: 0.8540 - val_loss: 0.5489 - val_accuracy: 0.7679\n",
      "Epoch 31/100\n",
      "678/678 [==============================] - 0s 150us/step - loss: 0.3473 - accuracy: 0.8673 - val_loss: 0.5583 - val_accuracy: 0.7381\n",
      "Epoch 32/100\n",
      "678/678 [==============================] - 0s 155us/step - loss: 0.3422 - accuracy: 0.8569 - val_loss: 0.5425 - val_accuracy: 0.7560\n",
      "Epoch 33/100\n",
      "678/678 [==============================] - 0s 125us/step - loss: 0.3367 - accuracy: 0.8584 - val_loss: 0.5440 - val_accuracy: 0.7619\n",
      "Epoch 34/100\n",
      "678/678 [==============================] - 0s 128us/step - loss: 0.3323 - accuracy: 0.8702 - val_loss: 0.5423 - val_accuracy: 0.7619\n",
      "Epoch 35/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.3304 - accuracy: 0.8643 - val_loss: 0.5354 - val_accuracy: 0.7798\n",
      "Epoch 36/100\n",
      "678/678 [==============================] - 0s 99us/step - loss: 0.3270 - accuracy: 0.8628 - val_loss: 0.5328 - val_accuracy: 0.7679\n",
      "Epoch 37/100\n",
      "678/678 [==============================] - 0s 83us/step - loss: 0.3195 - accuracy: 0.8879 - val_loss: 0.5369 - val_accuracy: 0.7679\n",
      "Epoch 38/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.3239 - accuracy: 0.8584 - val_loss: 0.5238 - val_accuracy: 0.7857\n",
      "Epoch 39/100\n",
      "678/678 [==============================] - 0s 83us/step - loss: 0.3141 - accuracy: 0.8791 - val_loss: 0.5395 - val_accuracy: 0.7560\n",
      "Epoch 40/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.3118 - accuracy: 0.8791 - val_loss: 0.5235 - val_accuracy: 0.7917\n",
      "Epoch 41/100\n",
      "678/678 [==============================] - 0s 81us/step - loss: 0.3074 - accuracy: 0.8805 - val_loss: 0.5202 - val_accuracy: 0.7560\n",
      "Epoch 42/100\n",
      "678/678 [==============================] - 0s 80us/step - loss: 0.3009 - accuracy: 0.8909 - val_loss: 0.5185 - val_accuracy: 0.7738\n",
      "Epoch 43/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.3039 - accuracy: 0.8879 - val_loss: 0.5208 - val_accuracy: 0.7798\n",
      "Epoch 44/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.2972 - accuracy: 0.8879 - val_loss: 0.5237 - val_accuracy: 0.7798\n",
      "Epoch 45/100\n",
      "678/678 [==============================] - 0s 88us/step - loss: 0.2921 - accuracy: 0.8835 - val_loss: 0.5193 - val_accuracy: 0.7619\n",
      "Epoch 46/100\n",
      "678/678 [==============================] - 0s 110us/step - loss: 0.2914 - accuracy: 0.8953 - val_loss: 0.5176 - val_accuracy: 0.7917\n",
      "Epoch 47/100\n",
      "678/678 [==============================] - 0s 120us/step - loss: 0.2923 - accuracy: 0.8997 - val_loss: 0.5227 - val_accuracy: 0.7679\n",
      "Epoch 48/100\n",
      "678/678 [==============================] - 0s 153us/step - loss: 0.2839 - accuracy: 0.8923 - val_loss: 0.5082 - val_accuracy: 0.7917\n",
      "Epoch 49/100\n",
      "678/678 [==============================] - 0s 145us/step - loss: 0.2836 - accuracy: 0.8879 - val_loss: 0.5152 - val_accuracy: 0.7857\n",
      "Epoch 50/100\n",
      "678/678 [==============================] - 0s 105us/step - loss: 0.2815 - accuracy: 0.8923 - val_loss: 0.5154 - val_accuracy: 0.7560\n",
      "Epoch 51/100\n",
      "678/678 [==============================] - 0s 83us/step - loss: 0.2783 - accuracy: 0.8953 - val_loss: 0.5012 - val_accuracy: 0.7798\n",
      "Epoch 52/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.2723 - accuracy: 0.9071 - val_loss: 0.5114 - val_accuracy: 0.7679\n",
      "Epoch 53/100\n",
      "678/678 [==============================] - 0s 80us/step - loss: 0.2716 - accuracy: 0.8982 - val_loss: 0.5009 - val_accuracy: 0.7857\n",
      "Epoch 54/100\n",
      "678/678 [==============================] - 0s 140us/step - loss: 0.2681 - accuracy: 0.9056 - val_loss: 0.5128 - val_accuracy: 0.7560\n",
      "Epoch 55/100\n",
      "678/678 [==============================] - 0s 185us/step - loss: 0.2712 - accuracy: 0.9027 - val_loss: 0.5187 - val_accuracy: 0.7619\n",
      "Epoch 56/100\n",
      "678/678 [==============================] - 0s 152us/step - loss: 0.2680 - accuracy: 0.9027 - val_loss: 0.5069 - val_accuracy: 0.7738\n",
      "Epoch 57/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "678/678 [==============================] - 0s 89us/step - loss: 0.2649 - accuracy: 0.8997 - val_loss: 0.4913 - val_accuracy: 0.7857\n",
      "Epoch 58/100\n",
      "678/678 [==============================] - 0s 83us/step - loss: 0.2616 - accuracy: 0.9027 - val_loss: 0.4916 - val_accuracy: 0.7857\n",
      "Epoch 59/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.2560 - accuracy: 0.9041 - val_loss: 0.5023 - val_accuracy: 0.7798\n",
      "Epoch 60/100\n",
      "678/678 [==============================] - 0s 86us/step - loss: 0.2521 - accuracy: 0.9130 - val_loss: 0.5110 - val_accuracy: 0.7857\n",
      "Epoch 61/100\n",
      "678/678 [==============================] - 0s 78us/step - loss: 0.2548 - accuracy: 0.8982 - val_loss: 0.4851 - val_accuracy: 0.7917\n",
      "Epoch 62/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.2548 - accuracy: 0.9100 - val_loss: 0.4963 - val_accuracy: 0.7738\n",
      "Epoch 63/100\n",
      "678/678 [==============================] - 0s 78us/step - loss: 0.2455 - accuracy: 0.9145 - val_loss: 0.4955 - val_accuracy: 0.7857\n",
      "Epoch 64/100\n",
      "678/678 [==============================] - 0s 78us/step - loss: 0.2429 - accuracy: 0.9204 - val_loss: 0.4941 - val_accuracy: 0.7917\n",
      "Epoch 65/100\n",
      "678/678 [==============================] - 0s 81us/step - loss: 0.2438 - accuracy: 0.9086 - val_loss: 0.4945 - val_accuracy: 0.7798\n",
      "Epoch 66/100\n",
      "678/678 [==============================] - 0s 87us/step - loss: 0.2416 - accuracy: 0.9056 - val_loss: 0.4891 - val_accuracy: 0.7857\n",
      "Epoch 67/100\n",
      "678/678 [==============================] - 0s 91us/step - loss: 0.2523 - accuracy: 0.8953 - val_loss: 0.4985 - val_accuracy: 0.7679\n",
      "Epoch 68/100\n",
      "678/678 [==============================] - 0s 128us/step - loss: 0.2403 - accuracy: 0.9115 - val_loss: 0.4915 - val_accuracy: 0.7857\n",
      "Epoch 69/100\n",
      "678/678 [==============================] - 0s 141us/step - loss: 0.2388 - accuracy: 0.9115 - val_loss: 0.4813 - val_accuracy: 0.7798\n",
      "Epoch 70/100\n",
      "678/678 [==============================] - 0s 122us/step - loss: 0.2323 - accuracy: 0.9248 - val_loss: 0.4986 - val_accuracy: 0.7857\n",
      "Epoch 71/100\n",
      "678/678 [==============================] - 0s 128us/step - loss: 0.2346 - accuracy: 0.9174 - val_loss: 0.5072 - val_accuracy: 0.7798\n",
      "Epoch 72/100\n",
      "678/678 [==============================] - 0s 96us/step - loss: 0.2295 - accuracy: 0.9086 - val_loss: 0.4907 - val_accuracy: 0.7976\n",
      "Epoch 73/100\n",
      "678/678 [==============================] - 0s 134us/step - loss: 0.2307 - accuracy: 0.9218 - val_loss: 0.4884 - val_accuracy: 0.7738\n",
      "Epoch 74/100\n",
      "678/678 [==============================] - 0s 130us/step - loss: 0.2261 - accuracy: 0.9218 - val_loss: 0.4906 - val_accuracy: 0.7857\n",
      "Epoch 75/100\n",
      "678/678 [==============================] - 0s 123us/step - loss: 0.2236 - accuracy: 0.9263 - val_loss: 0.4849 - val_accuracy: 0.7857\n",
      "Epoch 76/100\n",
      "678/678 [==============================] - 0s 107us/step - loss: 0.2211 - accuracy: 0.9322 - val_loss: 0.4965 - val_accuracy: 0.7738\n",
      "Epoch 77/100\n",
      "678/678 [==============================] - 0s 86us/step - loss: 0.2305 - accuracy: 0.9204 - val_loss: 0.4851 - val_accuracy: 0.7976\n",
      "Epoch 78/100\n",
      "678/678 [==============================] - 0s 84us/step - loss: 0.2203 - accuracy: 0.9248 - val_loss: 0.4833 - val_accuracy: 0.7917\n",
      "Epoch 79/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.2192 - accuracy: 0.9248 - val_loss: 0.4881 - val_accuracy: 0.7857\n",
      "Epoch 80/100\n",
      "678/678 [==============================] - 0s 82us/step - loss: 0.2195 - accuracy: 0.9189 - val_loss: 0.4894 - val_accuracy: 0.7857\n",
      "Epoch 81/100\n",
      "678/678 [==============================] - 0s 82us/step - loss: 0.2231 - accuracy: 0.9174 - val_loss: 0.4969 - val_accuracy: 0.7976\n",
      "Epoch 82/100\n",
      "678/678 [==============================] - 0s 125us/step - loss: 0.2150 - accuracy: 0.9263 - val_loss: 0.4888 - val_accuracy: 0.7738\n",
      "Epoch 83/100\n",
      "678/678 [==============================] - 0s 130us/step - loss: 0.2100 - accuracy: 0.9292 - val_loss: 0.4838 - val_accuracy: 0.7917\n",
      "Epoch 84/100\n",
      "678/678 [==============================] - 0s 121us/step - loss: 0.2092 - accuracy: 0.9336 - val_loss: 0.4789 - val_accuracy: 0.7976\n",
      "Epoch 85/100\n",
      "678/678 [==============================] - 0s 116us/step - loss: 0.2087 - accuracy: 0.9322 - val_loss: 0.4824 - val_accuracy: 0.7976\n",
      "Epoch 86/100\n",
      "678/678 [==============================] - 0s 90us/step - loss: 0.2058 - accuracy: 0.9336 - val_loss: 0.4847 - val_accuracy: 0.7917\n",
      "Epoch 87/100\n",
      "678/678 [==============================] - 0s 82us/step - loss: 0.2067 - accuracy: 0.9307 - val_loss: 0.4856 - val_accuracy: 0.7917\n",
      "Epoch 88/100\n",
      "678/678 [==============================] - 0s 79us/step - loss: 0.2028 - accuracy: 0.9366 - val_loss: 0.4831 - val_accuracy: 0.7798\n",
      "Epoch 89/100\n",
      "678/678 [==============================] - 0s 86us/step - loss: 0.2026 - accuracy: 0.9366 - val_loss: 0.4857 - val_accuracy: 0.7857\n",
      "Epoch 90/100\n",
      "678/678 [==============================] - 0s 83us/step - loss: 0.1992 - accuracy: 0.9322 - val_loss: 0.4809 - val_accuracy: 0.7857\n",
      "Epoch 91/100\n",
      "678/678 [==============================] - 0s 127us/step - loss: 0.1997 - accuracy: 0.9425 - val_loss: 0.4808 - val_accuracy: 0.7857\n",
      "Epoch 92/100\n",
      "678/678 [==============================] - 0s 120us/step - loss: 0.1977 - accuracy: 0.9322 - val_loss: 0.4909 - val_accuracy: 0.7917\n",
      "Epoch 93/100\n",
      "678/678 [==============================] - 0s 126us/step - loss: 0.2060 - accuracy: 0.9263 - val_loss: 0.4675 - val_accuracy: 0.7917\n",
      "Epoch 94/100\n",
      "678/678 [==============================] - 0s 119us/step - loss: 0.1983 - accuracy: 0.9410 - val_loss: 0.4852 - val_accuracy: 0.7857\n",
      "Epoch 95/100\n",
      "678/678 [==============================] - 0s 111us/step - loss: 0.1945 - accuracy: 0.9351 - val_loss: 0.4849 - val_accuracy: 0.7798\n",
      "Epoch 96/100\n",
      "678/678 [==============================] - 0s 89us/step - loss: 0.1965 - accuracy: 0.9277 - val_loss: 0.4829 - val_accuracy: 0.7857\n",
      "Epoch 97/100\n",
      "678/678 [==============================] - 0s 84us/step - loss: 0.1936 - accuracy: 0.9351 - val_loss: 0.4746 - val_accuracy: 0.7857\n",
      "Epoch 98/100\n",
      "678/678 [==============================] - 0s 102us/step - loss: 0.1913 - accuracy: 0.9410 - val_loss: 0.4688 - val_accuracy: 0.7917\n",
      "Epoch 99/100\n",
      "678/678 [==============================] - 0s 130us/step - loss: 0.1961 - accuracy: 0.9336 - val_loss: 0.4768 - val_accuracy: 0.7976\n",
      "Epoch 100/100\n",
      "678/678 [==============================] - 0s 134us/step - loss: 0.1878 - accuracy: 0.9454 - val_loss: 0.4847 - val_accuracy: 0.7857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x1a3eb43790>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train Model1\n",
    "np.random.seed(12345) # do not change it!\n",
    "model1.fit(X_train_sc, Y_01_train, epochs=100, validation_data = (X_test_sc, Y_01_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.7514957e-01, 2.0808069e-02, 3.6627930e-03, 3.7960117e-04],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_01_test_pred = model1.predict(X_test_sc)\n",
    "\n",
    "Y_01_test_pred[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97        39\n",
      "           1       0.68      0.57      0.62        47\n",
      "           2       0.64      0.65      0.64        43\n",
      "           3       0.88      0.95      0.91        39\n",
      "\n",
      "   micro avg       0.79      0.77      0.78       168\n",
      "   macro avg       0.79      0.79      0.79       168\n",
      "weighted avg       0.78      0.77      0.78       168\n",
      " samples avg       0.77      0.77      0.77       168\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/envs/tensorflow_env/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1268: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# prediction of classes\n",
    "\n",
    "Y_01_test_pred = model1.predict(X_test_sc)\n",
    "\n",
    "Y_01_test_pred[1]\n",
    "# classification report\n",
    "print(classification_report(y_true=Y_01_test, y_pred=Y_01_test_pred.round(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Give your comments here**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 (2 points). PCA analysis\n",
    "\n",
    "- Run a PCA analysis on the scaled features.\n",
    "- Calculate variances and cumulative variances.\n",
    "- For further analysis, leave only first 9 principal components. How much variance of the original dataset do they explain?\n",
    "- Give brief comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PCA(copy=True, iterated_power='auto', n_components=None, random_state=None,\n",
       "    svd_solver='auto', tol=0.0, whiten=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run a PCA\n",
    "pca = skPCA(n_components=None)\n",
    "pca.fit(X_train_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5.23679614e-01, 1.72827778e-01, 1.01457506e-01, 6.67645018e-02,\n",
       "       4.89127117e-02, 2.97375344e-02, 1.98983675e-02, 1.22139920e-02,\n",
       "       8.82540635e-03, 5.09920830e-03, 3.22658195e-03, 2.42373565e-03,\n",
       "       1.95514390e-03, 1.07875398e-03, 8.29131024e-04, 7.09152954e-04,\n",
       "       3.41093682e-04, 1.97865794e-05])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# % of variance explained by PCs\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.52367961, 0.69650739, 0.7979649 , 0.8647294 , 0.91364211,\n",
       "       0.94337965, 0.96327801, 0.97549201, 0.98431741, 0.98941662,\n",
       "       0.9926432 , 0.99506694, 0.99702208, 0.99810084, 0.99892997,\n",
       "       0.99963912, 0.99998021, 1.        ])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cumulative variance\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "cum_var_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make first 9 components - for the training and test samples\n",
    "PC1_loadings = pca.components_[0] \n",
    "#for the first component\n",
    "\n",
    "PC2_loadings = pca.components_[1] \n",
    "PC3_loadings = pca.components_[2] \n",
    "PC4_loadings = pca.components_[3] \n",
    "PC5_loadings = pca.components_[4] \n",
    "PC6_loadings = pca.components_[5] \n",
    "PC7_loadings = pca.components_[6] \n",
    "PC8_loadings = pca.components_[7] \n",
    "PC9_loadings = pca.components_[8] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC1_values = pca.transform(X_train_sc)[:,0]\n",
    "#taking the first column out of we got\n",
    "PC2_values = pca.transform(X_train_sc)[:,1]\n",
    "PC3_values = pca.transform(X_train_sc)[:,2]\n",
    "PC4_values = pca.transform(X_train_sc)[:,3]\n",
    "PC5_values = pca.transform(X_train_sc)[:,4]\n",
    "PC6_values = pca.transform(X_train_sc)[:,5]\n",
    "PC7_values = pca.transform(X_train_sc)[:,6]\n",
    "PC8_values = pca.transform(X_train_sc)[:,7]\n",
    "PC9_values = pca.transform(X_train_sc)[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29880345,  3.74437669, -1.81719983,  0.6602405 ,  6.42285853,\n",
       "       -2.19403958, -4.54912514, -3.5591212 , -0.86214409, -1.44927976,\n",
       "       -5.29704398,  0.29854454,  4.14919613,  4.21121305, -1.39708005,\n",
       "       -5.10742168, -1.2257356 , -2.80504379,  3.83113545, -4.83204158,\n",
       "        4.56827006,  1.51589253, -3.22483961, -0.91641204, -2.09614437,\n",
       "        3.86839488, -1.24747588, -2.24258934,  3.15538668,  4.00617063,\n",
       "        2.95053715, -5.33842343, -0.2325941 , -2.80650546, -0.21365254,\n",
       "       -3.86960044,  4.51306847, -1.1963665 ,  6.04490363, -3.18832071,\n",
       "        1.16971839, -0.85383792, -4.04562329, -2.18062866,  1.10980907,\n",
       "       -2.23684527, -2.40816134,  4.90266082,  2.26316907,  3.59064047,\n",
       "        5.47418032,  0.04618475, -5.28195135, -1.21020777,  3.00073697,\n",
       "       -0.66239272, -2.67991669, -2.14694032, -1.38986924,  3.36522808,\n",
       "       -3.89013461, -1.57235526,  6.79012108, -1.61696294, -4.90249709,\n",
       "       -4.98898884,  4.93997331, -0.87333615,  3.32523504,  0.04090637,\n",
       "       -2.09299572,  3.96924145, -1.85291124,  0.98926029, -0.99255879,\n",
       "       -2.6222737 , -4.46777151,  1.68543679,  4.62997831,  2.4823964 ,\n",
       "       -2.43567667, -2.2417925 ,  1.37605518, -2.07223887, -0.33909455,\n",
       "       -3.00895557, -5.349943  , -1.12960671,  0.08839204,  2.60544132,\n",
       "       -2.36028017,  0.87787693, -4.05710644, -0.50995328, -1.75815735,\n",
       "       -4.465217  , -0.60033461, -2.08349593,  2.68238335,  2.4336631 ,\n",
       "       -2.37120214,  1.31253084, -3.2469974 ,  3.68909058,  2.97917637,\n",
       "       -4.08128415, -1.7288198 ,  3.71422958, -2.50643825,  0.09310175,\n",
       "        0.22430964, -1.7769406 , -2.79057038,  2.54271471,  1.16295988,\n",
       "       -2.22791005,  3.64083946, -2.03386616, -2.60423819, -3.06376985,\n",
       "       -2.18889021, -1.27800856, -4.45346116,  3.74263175, -0.51195776,\n",
       "        3.89278123,  3.89011905, -2.06859224,  3.94338272, -2.25630696,\n",
       "       -3.43621281, -0.9448198 ,  3.07487935, -0.14660149, -1.18369652,\n",
       "       -1.63628041, -1.82659269, -2.50044768,  4.47424031, -5.31484002,\n",
       "       -1.74020863, -5.39231694,  3.08516725, -0.32821458, -0.32298278,\n",
       "        3.9826404 , -1.7724024 ,  1.76317655, -1.85776499,  5.49328285,\n",
       "       -1.37130249, -2.12713919,  5.20765365, -1.71583709, -1.82419597,\n",
       "       -2.08485144, -0.26324127, -2.00213995,  4.45762326,  4.19742392,\n",
       "       -3.70477916, -1.21081435, -0.26588799, -2.6266173 , -1.61957325,\n",
       "        3.84621812, -0.21541429, -1.65572726,  2.84860452, -1.76526265,\n",
       "        4.96841936, -2.10063515, -4.52806745, -0.86520584,  1.94053359,\n",
       "       -1.8905641 , -3.19576525, -1.72847112,  4.42301852, -3.47461556,\n",
       "        4.37595007, -2.41534431, -6.16412023, -2.43642762,  3.43846448,\n",
       "       -2.17934676, -0.57697645, -1.02211035, -2.1249426 ,  4.45072949,\n",
       "       -2.00739908, -0.50757114, -2.60676151,  4.65161529, -2.78382438,\n",
       "       -1.57685799, -0.91719982,  4.60148368, -3.39162486,  5.43602216,\n",
       "       -1.20421596, -2.21367766, -1.44525965,  2.77047308,  4.89836518,\n",
       "       -4.66325729, -4.00074145, -4.07605158,  2.32248262, -2.13659354,\n",
       "        0.55680225, -4.07465221, -4.96658373, -2.02822694,  1.02325469,\n",
       "       -1.11651472, -1.1283035 ,  6.34217064, -1.47265018, -1.91198938,\n",
       "       -3.36599766, -1.77138156,  1.10143339, -1.89298568, -1.81318184,\n",
       "        5.42672406, -1.09262163,  0.51258321, -2.38953632, -1.44485969,\n",
       "       -3.87619774, -3.31456337,  1.44235061, -1.56607604,  5.42721804,\n",
       "        2.59830278,  0.44447412, -0.28745197,  4.16338038, -1.13486101,\n",
       "       -1.41593224, -3.30340626, -1.62882676,  3.62841392,  3.63011545,\n",
       "        2.96232382,  3.57448725,  1.75006479, -0.5226363 , -2.2666979 ,\n",
       "        3.61992759, -2.28494384, -3.37076056, -3.13052314,  1.84528177,\n",
       "       -5.04580639,  3.85804662,  4.68097084,  7.3105422 ,  2.62067721,\n",
       "       -0.51607565, -2.44464598,  2.20930218, -1.10618992, -1.49925585,\n",
       "       -1.2623804 ,  2.56569917,  5.39133691, -2.54949592,  5.22937711,\n",
       "        4.95168428, -1.41312499, -0.47305441, -1.96438131, -2.19485504,\n",
       "       -4.30847767,  4.28715463,  4.90457048,  5.22398843,  5.21548082,\n",
       "       -1.117793  , -1.09774775, -2.82540456,  2.01295404, -3.10450011,\n",
       "       -0.92433338, -1.68902085,  1.48662812, -1.02613611,  3.94751208,\n",
       "        2.03826422,  4.4704056 ,  2.74598331,  0.26480227, -0.80424566,\n",
       "       -1.30947445,  4.01669309, -0.67077122, -3.96958407, -0.31767771,\n",
       "       -0.21805278, -0.15601983,  0.60367021, -5.16962067,  4.42156921,\n",
       "        4.19199006, -4.10895412, -5.03873725,  4.54152613, -4.10425923,\n",
       "        6.28570352, -1.65215614,  4.37309781, -2.07832786,  0.04857851,\n",
       "        1.22630656,  5.08787081, -2.57905636, -2.20686622, -0.68512522,\n",
       "       -1.02162987,  1.07746767,  6.54767974, -1.84378032, -0.90531611,\n",
       "       -2.75483756,  4.1811636 , -2.2774462 ,  3.43800079, -1.986612  ,\n",
       "        2.85865909, -4.11033144, -2.81517098, -1.71275026, -0.84235923,\n",
       "       -0.27136156, -2.94980614, -1.01343071, -1.9510006 , -1.85085924,\n",
       "       -1.46451098, -3.46004417,  2.89209967, -3.35415757, -2.14537869,\n",
       "        0.57784832, -3.44906534,  4.56407306, -0.78024188,  3.84513637,\n",
       "        2.71333672, -2.35923224, -2.12946917, -3.77060197, -2.94028849,\n",
       "        1.22912338,  2.58438598, -0.82013307, -4.6343113 ,  2.22955463,\n",
       "       -1.08759278,  2.76624643,  4.43693851, -1.72903835, -0.44248784,\n",
       "       -1.53324529, -1.18366811, -1.53178544,  3.43977634, -2.32425247,\n",
       "       -1.81031668,  5.09748261,  0.40820721,  3.61109031,  0.41472481,\n",
       "       -2.85106025,  1.78107998, -1.9563521 , -2.328754  ,  4.46251095,\n",
       "       -3.1870129 , -2.23717955,  5.22888016,  3.64045949,  5.06449791,\n",
       "       -2.47502021,  4.26140535,  4.43494996, -0.66718   ,  1.41304717,\n",
       "       -2.69253428,  4.07051239,  0.2349474 , -0.60152504, -1.42195214,\n",
       "       -2.38299636, -0.24598759, -2.37138115,  4.07959199,  5.4069516 ,\n",
       "       -0.58140772,  4.62141315, -1.28312466, -5.19902188,  4.74262118,\n",
       "        4.26713919, -4.01075867,  2.17426257,  4.95816297, -2.23283487,\n",
       "        0.04168454, -2.40069268, -1.79014213,  4.5304228 , -4.46616736,\n",
       "       -1.21012661,  5.2347321 , -0.23585891, -2.97680998, -2.88970223,\n",
       "        0.67419813,  4.21356723, -0.65524892,  1.63227954,  0.51705164,\n",
       "        0.89363675,  3.32662319, -3.84839661, -3.74896269, -0.24013859,\n",
       "        4.86095844, -3.64461746,  2.85308559,  5.54311815, -4.20509278,\n",
       "       -0.78248108, -1.26093616, -1.14903822, -1.03174622, -1.95336584,\n",
       "       -1.41228768,  3.79237775, -4.81902324, -3.19165683, -2.21824515,\n",
       "        3.33232424,  1.08437602,  6.13235983, -3.25173843, -0.60887855,\n",
       "        4.13250763, -1.93102288, -1.79838568,  1.33571569,  4.41757788,\n",
       "       -1.19445659, -2.35748741, -3.62540732,  4.80372274, -1.14013935,\n",
       "       -0.52157876, -0.80333345,  4.20347723,  4.6491247 ,  4.75682427,\n",
       "        3.5727007 , -0.18427352, -4.07924436, -2.58250436,  4.4644675 ,\n",
       "        4.22885157,  2.83350674, -4.85748939, -3.44140773, -2.6006764 ,\n",
       "        5.2794769 , -2.31750299, -4.41755122,  1.17192619, -0.38308839,\n",
       "       -3.45044818,  0.49647084, -1.23130547,  4.51659491, -3.19640582,\n",
       "       -4.28772069, -3.73749884, -2.76504972,  4.1541469 , -0.30442853,\n",
       "       -1.34048403, -3.39539332, -2.21376169, -5.04928851, -3.06119683,\n",
       "        3.49981555,  5.26126314, -0.84302713,  0.53953701,  3.86881003,\n",
       "       -2.10748013, -1.78563834,  2.01580423, -2.73031707, -3.41307175,\n",
       "        3.09282897, -1.94119325,  3.2145568 , -3.05259397, -1.95679718,\n",
       "       -2.13717797,  1.23844472,  4.96505489,  0.32830595, -0.47898977,\n",
       "       -1.72254754, -1.30987744, -2.78975806,  3.9864063 , -2.1322977 ,\n",
       "        3.90229405, -1.16041775,  0.38131385,  1.12104805, -5.74605902,\n",
       "       -1.77657256, -0.57069372, -2.51387228,  2.51430565, -4.13178008,\n",
       "        2.24154904, -0.10221469, -2.08315215, -0.13021638,  1.18715219,\n",
       "        4.44225402,  0.28707767, -2.25862432,  2.57932827,  0.50640186,\n",
       "        1.89552331, -1.67629883, -1.67223445,  2.55243257, -2.10070243,\n",
       "       -1.24482011, -0.39064387, -0.8897084 ,  6.57411075, -2.67456641,\n",
       "        7.436968  , -5.10345023,  2.93939753,  3.32772755, -0.93678576,\n",
       "       -2.9229268 , -2.17369391, -1.75777926, -2.21377956,  0.13130861,\n",
       "        3.58303852,  0.77577471,  3.37171523,  4.61003354, -4.36435587,\n",
       "        0.6356304 ,  4.63420477,  3.69852731,  2.47927989,  4.72589296,\n",
       "       -0.74511925,  4.83668822,  0.05011534,  0.31551045,  4.40538379,\n",
       "        4.96552603, -0.31450113,  2.01163646, -0.49345502, -3.41011628,\n",
       "        2.93873022, -0.62720136, -4.2586457 ,  4.79583272,  6.8130564 ,\n",
       "        3.5333577 ,  3.54668744,  3.61852805, -0.72145027,  4.19202281,\n",
       "       -2.18307823,  2.37194699, -0.148828  , -2.61844791, -2.46304089,\n",
       "       -0.7373177 ,  2.18011477, -0.18369409, -5.38334403, -0.96528129,\n",
       "       -2.34086657, -2.79504982, -4.55332963,  2.85728336, -2.04153174,\n",
       "       -0.24244852,  1.8111425 ,  4.02902559,  3.95258889, -2.23425514,\n",
       "        2.05964294, -2.38818595, -2.25071819,  2.5491959 , -2.17799823,\n",
       "       -3.09889256, -0.70428459,  1.98661844,  4.56348345, -3.42560698,\n",
       "        2.82500803, -2.8098924 ,  1.38061021,  4.32796079, -5.03633003,\n",
       "       -2.51914153,  4.85771319, -3.73632895,  4.49772177, -1.21916367,\n",
       "       -2.58733004, -2.33334622,  2.91923484, -1.84601938, -4.2361425 ,\n",
       "        4.36835725,  4.20929342, -1.58404092, -1.80209663, -0.22807278,\n",
       "        1.25043332,  3.21491529, -2.20685904, -1.19114793,  3.60108839,\n",
       "       -1.8735313 , -4.13813641,  4.10444373, -3.45669276, -0.84274565,\n",
       "       -2.31805908, -1.34365935, -0.01089759,  4.77205255,  1.50711923,\n",
       "       -3.65352878,  4.92389268, -1.47487922,  3.83985686, -1.71680288,\n",
       "        1.2179906 ,  4.56759654, -2.96495439, -0.29189751, -2.02979433,\n",
       "        0.15512235,  4.18201609, -0.53174794, -4.60788502,  7.06933107,\n",
       "       -2.18302235, -1.30587582, -1.9921077 , -3.83141548, -0.48299729,\n",
       "        4.78617897, -3.35264277, -4.80149613])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PC1_values = pca.transform(X_train_sc)[:,0]\n",
    "#taking the first column out of we got\n",
    "PC2_values = pca.transform(X_train_sc)[:,1]\n",
    "PC3_values = pca.transform(X_train_sc)[:,2]\n",
    "PC4_values = pca.transform(X_train_sc)[:,3]\n",
    "PC5_values = pca.transform(X_train_sc)[:,4]\n",
    "PC6_values = pca.transform(X_train_sc)[:,5]\n",
    "PC7_values = pca.transform(X_train_sc)[:,6]\n",
    "PC8_values = pca.transform(X_train_sc)[:,7]\n",
    "PC9_values = pca.transform(X_train_sc)[:,8]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "307px",
    "left": "909px",
    "right": "20px",
    "top": "120px",
    "width": "338px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
